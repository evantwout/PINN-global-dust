{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yl93P7bU_XDm"
   },
   "source": [
    "# Sensitivity analysis of the PINN\n",
    "\n",
    "This notebook trains the Physics-Informed Neural Network (PINN) on the empirical dataset corresponding to dust flux depositions for the Holocene and Last Glacial Maximum. Different values of the weight parameter for the model loss are taken to study the paramter sensitivity of the PINN.\n",
    "\n",
    "The preprocessing codes should have been performed before.\n",
    "\n",
    "The training of the PINN may take several hours, depending on the computer facilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pwRJ8raWO6NK"
   },
   "source": [
    "## Preliminaries\n",
    "\n",
    "Import the necessary libraries and specify the data folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vml3l0PulZNT",
    "outputId": "b9ee6a44-2bef-4f82-d2b9-9cff05e710db"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import deepxde as dde\n",
    "import geopandas as gpd\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from scipy.stats import norm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from deepxde.backend import tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TPLAcsp3PGlr"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W950y6a_O6NM"
   },
   "outputs": [],
   "source": [
    "DATA_PATH = \"../Data/\"\n",
    "INPUT_MODEL_PATH = DATA_PATH + \"processed_data/\"\n",
    "MODEL_SAVE_PATH = DATA_PATH + \"trained_models/\"\n",
    "RESULTS_PATH = DATA_PATH + \"model_results/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TbSNmZ-fuTAw"
   },
   "outputs": [],
   "source": [
    "world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BpKMFRKlJh8K"
   },
   "source": [
    "Load functions for training the PINN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "32s_pLRNJjQ1"
   },
   "outputs": [],
   "source": [
    "with open(\"functions_training_model.py\", 'r') as file:\n",
    "    content = file.read()\n",
    "\n",
    "# Execute the content of the .py file\n",
    "exec(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kFInF39yD9Bo"
   },
   "source": [
    "## Load the datasets\n",
    "\n",
    "Load the datasets with dust flux depositions in the Holocene and LGM periods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CamP0hOzO6NO"
   },
   "outputs": [],
   "source": [
    "df_empirical_Holocene = pd.read_csv(INPUT_MODEL_PATH + \"df_empirical_Holocene.csv\")\n",
    "df_empirical_LGM = pd.read_csv(INPUT_MODEL_PATH + \"df_empirical_LGM.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xaeScI1NO6NP"
   },
   "source": [
    "Load the global grid with a 3 degrees resolution on which the dust flux deposition will be reconstructed with the PINN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PRIr75V7O6NP"
   },
   "outputs": [],
   "source": [
    "df_global_grid = pd.read_csv(INPUT_MODEL_PATH + \"df_global_grid.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JNZru0umsmPd"
   },
   "source": [
    "## Specify the model settings\n",
    "\n",
    "The following steps are needed to set all the variables for the problem.\n",
    "1. Incorporate the wind directions.\n",
    "2. Define the training points.\n",
    "3. Define the equation with the domain and the boundary conditions.\n",
    "4. Define the neural network architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H4vx0yFDZcpb"
   },
   "source": [
    "### 1. Load the processed wind data\n",
    "\n",
    "The advection term includes the latitude-dependent average wind speed. The latitude and wind speed have to be normalized to be included in the PINN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cVq1MnC8A9xq"
   },
   "outputs": [],
   "source": [
    "df_wind = pd.read_csv(INPUT_MODEL_PATH + \"df_wind.csv\", usecols=['wind', 'latitude'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "prhVlY_PLwPM"
   },
   "outputs": [],
   "source": [
    "latitude_wind, mean_wind = df_wind['latitude'].values/90, df_wind['wind'].values/df_wind['wind'].max()\n",
    "\n",
    "def wind_latitude(latitude):\n",
    "    interpolated = wind_tf_interp(latitude, tf.convert_to_tensor(latitude_wind), tf.convert_to_tensor(mean_wind))\n",
    "    return interpolated\n",
    "\n",
    "tf_wind_latitude = tf.function(wind_latitude)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qpQHxmZtujGy"
   },
   "source": [
    "### 2. Define the training points\n",
    "\n",
    "The training points are the locations for wich dust deposition data is available. The latitude and longitude are normalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ez0qBGVwO6NQ"
   },
   "outputs": [],
   "source": [
    "def training_points(df):\n",
    "    \"\"\"\n",
    "    Create training data and boundary conditions for a deep learning model.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pandas DataFrame): The input DataFrame containing longitude (lon), latitude (lat), and normalized log deposition data (log_dep_norm) for the training process.\n",
    "\n",
    "    Returns:\n",
    "    - data_observ_points (dde.data.DataSet): The DataSet object containing the training locations where dust measurements are taken.\n",
    "    - observe_u (dde.icbc.PointSetBC): The observed values representing the dust deposition.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a DataSet object\n",
    "    data_observ_points = dde.data.DataSet(\n",
    "        X_train=df[['lon', 'lat']].values / 90,\n",
    "        y_train=df['log_dep_norm'].values.reshape(-1, 1),\n",
    "        X_test=df[['lon', 'lat']].values / 90,\n",
    "        y_test=df['log_dep_norm'].values.reshape(-1, 1),\n",
    "        standardize=False)\n",
    "\n",
    "    # Define boundary conditions for observed values\n",
    "    observe_u = dde.icbc.PointSetBC(\n",
    "        data_observ_points.train_x,\n",
    "        df['log_dep_norm'].values.reshape(-1, 1),\n",
    "        component=0)\n",
    "\n",
    "    return data_observ_points, observe_u\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zC-VUycebmQ3"
   },
   "source": [
    "### 3. Define the equation with the domain and the boundary conditions\n",
    "\n",
    "We model the Earth's surface as a sphere and write the advection-diffusion equation in spherical coordinates. Let denote the longitude by $\\lambda \\in (-\\pi, \\pi)$ and the latitude by $\\theta \\in (-\\frac{\\pi}{2}, -\\frac{\\pi}{2})$ in radians. The advection-diffusion equation is defined as\n",
    "\\begin{align}\n",
    "\t\\frac{v_1}{\\cos(\\theta)} \\frac{\\partial u}{\\partial \\lambda} - D \\left( \\frac{1}{\\cos^2(\\theta)} \\frac{\\partial^2 u}{\\partial \\lambda^2} + \\frac{\\partial^2 u}{\\partial \\theta ^2} - \\tan(\\theta) \\frac{\\partial u}{\\partial \\theta} \\right) &= 0,\n",
    "\\end{align}\n",
    "with the periodic boundary conditions\n",
    "\\begin{align}\n",
    "\\begin{cases}\n",
    "\tu(-\\pi, \\theta) = u(\\pi, \\theta) ,\\\\\n",
    "\t\\frac{\\partial u (-\\pi, \\theta)}{\\partial \\lambda} = \\frac{\\partial u  (\\pi, \\theta)}{\\partial \\lambda}.\n",
    "\\end{cases}\n",
    "\\end{align}\n",
    "in longitudinal direction. The values at the poles are prescribed as\n",
    "\\begin{cases}\n",
    "\tu(\\lambda, -\\frac{\\pi}{2}) = u_\\text{south}, \\\\\n",
    "\tu(\\lambda, \\frac{\\pi}{2}) = u_\\text{north}.\n",
    "\\end{cases}\n",
    "Notice that due to the singularity at the poles, the training domain will be taken from -81 to 81 degrees latitude. No measurements sites are located in the excluded polar zones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5yZQGGQV9xdF"
   },
   "outputs": [],
   "source": [
    "x_min, x_max = -2.0, 2.0\n",
    "y_min, y_max = -0.89, 0.89\n",
    "\n",
    "left_corner = np.array([x_min, y_min]) # xmin, ymin – Coordinate of bottom left corner.\n",
    "right_corner = np.array([x_max, y_max]) # xmax, ymax – Coordinate of top right corner.\n",
    "geometry_rectangle = dde.geometry.geometry_2d.Rectangle(left_corner, right_corner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rnk3u4_FLwLt"
   },
   "outputs": [],
   "source": [
    "D = dde.Variable(1.0)\n",
    "\n",
    "def pde(x, u):\n",
    "    du_x = dde.grad.jacobian(u, x, j=0) # du/dlambda\n",
    "    du_y = dde.grad.jacobian(u, x, j=1) # du/dtheta\n",
    "\n",
    "    K = wind_latitude(x[:, 1:2])\n",
    "    K = tf.cast(K, tf.float32)\n",
    "\n",
    "    du_xx = dde.grad.hessian(u, x, i=0, j=0) # d^2u/dlambda^2\n",
    "    du_yy = dde.grad.hessian(u, x, i=1, j=1) # d^2u/dtheta^2\n",
    "\n",
    "    return (\n",
    "        ( -K * du_x * (1/tf.cos(x[:, 1:2]* np.pi /2)) +\n",
    "        D *( (1/(tf.cos(x[:, 1:2]* np.pi /2)**2) * du_xx + du_yy - tf.tan(x[:, 1:2]* np.pi /2) * du_y)) )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LYOKDP3C4L8B"
   },
   "outputs": [],
   "source": [
    "def space_boundary_north(x, on_boundary):\n",
    "    return on_boundary and np.isclose(y_max, x[1])\n",
    "\n",
    "def space_boundary_south(x, on_boundary):\n",
    "    return on_boundary and np.isclose(y_min, x[1])\n",
    "\n",
    "def periodic_boundary(x, domain):\n",
    "    return domain and (np.isclose(x[0], x_min) or np.isclose(x[0], x_max))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B21USxArvLQw"
   },
   "source": [
    "### 4. Define the neural network architecture\n",
    "\n",
    "The following model parameters need to be set.\n",
    "\n",
    "* `data_observ_points` and `observe_u`: The training locations and the dust measurements.\n",
    "\n",
    "* `MODEL_NAME_TO_SAVE`: The name under which the trained model will be saved.\n",
    "\n",
    "* `geometry_rectangle`: The geometry object defining the domain of the PDE problem.\n",
    "\n",
    "* `pde`: The PDE equation function defining the PDE problem to be solved.\n",
    "\n",
    "* `periodic_condition`, `periodic_condition_derivative`, `bc_1`, and `bc_2`: These are functions representing different boundary conditions.\n",
    "\n",
    "* `num_domain` and `num_boundary`: The number of domain and boundary points to use in the training data.\n",
    "\n",
    "* `neurons`, `layer`, `layer_size`, `activation`, and `initializer`: These parameters define the architecture of the neural network model (FNN) used to approximate the solution. They determine the number of hidden neurons, layers, activation function, and weight initialization method.\n",
    "\n",
    "* `epochs`: The number of training epochs to run.\n",
    "\n",
    "* `lr`: Learning rate for the Adam optimizer.\n",
    "\n",
    "* `D`: This variable represents the diffusion coefficient. It's a hyperparameter that affects the rate of diffusion in the PDE problem. This value is calculated by the network as an inverse problem\n",
    "\n",
    "* `external_trainable_variables`: A list of external variables (like `D`, `north_mean`, `south_mean`) that the model should optimize during training.\n",
    "\n",
    "* `loss_weights`: Weights applied to different loss terms in the optimization objective.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xZRPBwSAANG3"
   },
   "outputs": [],
   "source": [
    "def train_process(data_observ_points, observe_u, D, bc_1, bc_2, model_name, pde_weight):\n",
    "    data = dde.data.PDE(\n",
    "        geometry_rectangle,\n",
    "        pde,\n",
    "        [observe_u, periodic_condition, periodic_condition_derivative, bc_1, bc_2],\n",
    "        num_domain=2592,\n",
    "        num_boundary=216,\n",
    "        anchors=data_observ_points.train_x,\n",
    "        train_distribution='uniform'\n",
    "    )\n",
    "\n",
    "    neurons = 32\n",
    "    layer = 5\n",
    "    layer_size = [2] + [neurons] * layer + [1]\n",
    "    activation = \"selu\"\n",
    "    initializer = \"Glorot normal\"\n",
    "    net = dde.maps.FNN(layer_size, activation, initializer)\n",
    "    model = dde.Model(data, net)\n",
    "    dde.optimizers.set_LBFGS_options(maxcor=50, ftol=1e-20, maxiter=1e5)\n",
    "    model.compile(\"adam\", lr=0.00001, external_trainable_variables=[D, north_mean, south_mean],\n",
    "                  loss_weights=[pde_weight, 10, 0.5, 0.5, 1, 1])\n",
    "\n",
    "    # Train and save the model\n",
    "    checkpointer = dde.callbacks.ModelCheckpoint(\n",
    "        f\"{MODEL_SAVE_PATH}{model_name}/{model_name}.ckpt\",\n",
    "        verbose=0, period=10000,\n",
    "    )\n",
    "\n",
    "    variable = dde.callbacks.VariableValue([D, north_mean, south_mean], period=10000,\n",
    "                                           filename=MODEL_SAVE_PATH+model_name+\"/variables.dat\")\n",
    "\n",
    "    losshistory, train_state = model.train(epochs=150000, callbacks=[variable, checkpointer])\n",
    "    dde.saveplot(losshistory, train_state, issave=False, isplot=True)\n",
    "    params = variable.get_value()\n",
    "\n",
    "    return model, params, train_state.best_step\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I-dDklu7O6NR"
   },
   "source": [
    "Define the model weights for the sensitivity analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PUHo5DrNO6NR"
   },
   "outputs": [],
   "source": [
    "weights = [0, 0.1, 10, 1000]\n",
    "\n",
    "with open(RESULTS_PATH + \"weights_sensitivity.csv\", 'w') as f:\n",
    "    pd.DataFrame(data=weights, columns=['weights']).to_csv(f, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LFjYfhf2MRo9"
   },
   "source": [
    "## Perform sensitivity analysis of the PINN for the Holocene\n",
    "\n",
    "Perform the training process for the Holocene, for each of the model weights.\n",
    "\n",
    "Notice that training the neural network is resource expensive and may take minutes to hours depending on the computer facilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "BGsvDa39MQ7n",
    "outputId": "126cd916-69e4-4458-fe5a-b1b884d3511d"
   },
   "outputs": [],
   "source": [
    "models_list = []\n",
    "list_name = []\n",
    "\n",
    "for idx, pde_weight in enumerate(weights):\n",
    "\n",
    "    north_mean = dde.Variable(-1.0)\n",
    "    south_mean = dde.Variable(-2.0)\n",
    "    D = dde.Variable(1.0)\n",
    "\n",
    "    bc_1 = dde.DirichletBC(geometry_rectangle, lambda x: north_mean, space_boundary_north)\n",
    "    bc_2 = dde.DirichletBC(geometry_rectangle, lambda x: south_mean, space_boundary_south)\n",
    "\n",
    "    periodic_condition = dde.icbc.PeriodicBC(geom = geometry_rectangle, component_x = 0, on_boundary = periodic_boundary, derivative_order= 0)\n",
    "    periodic_condition_derivative = dde.icbc.PeriodicBC(geom = geometry_rectangle, component_x = 0, on_boundary = periodic_boundary, derivative_order= 1)\n",
    "\n",
    "    data_observ_points_Holocene, observe_u_Holocene = training_points(df_empirical_Holocene)\n",
    "\n",
    "    name = 'model_empirical_Holocene_' + str(idx)\n",
    "    model_empirical_Holocene, params, best_step = train_process(data_observ_points_Holocene, observe_u_Holocene, D, bc_1, bc_2, name, pde_weight)\n",
    "\n",
    "    models_list.append(model_empirical_Holocene)\n",
    "    list_name.append(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A5BJl565zSPR"
   },
   "source": [
    "Apply the PINN to a global grid to reconstruct the dust deposition rates and save the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7qXyLMZp2jrW"
   },
   "outputs": [],
   "source": [
    "def calculate_save_df(model, df_to_predict, mean, std, path, filename):\n",
    "    \"Perform the predictions and save the results.\"\n",
    "    U_pred = model.predict(df_to_predict[['lon', 'lat']].values/90)\n",
    "    U_pred_denorm = (U_pred + mean)*std\n",
    "    # Add the value of the prediction of the PINN in the specific location\n",
    "    df_to_predict['PINN_log_dep'] = U_pred_denorm\n",
    "\n",
    "    # Save the dataset with the prediction\n",
    "    with open(path+filename, 'w') as f:\n",
    "        df_to_predict.to_csv(f, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1tzUPmCrO6NS"
   },
   "outputs": [],
   "source": [
    "for idx, model in enumerate(models_list):\n",
    "\n",
    "    mean = df_empirical_Holocene['log_dep'].mean()\n",
    "    std = df_empirical_Holocene['log_dep'].std()\n",
    "\n",
    "    # PINN reconstruction on the global grid\n",
    "    calculate_save_df(model, df_global_grid, mean, std, RESULTS_PATH, \"df_pinn_empirical_Holocene_sensitivity_\"+str(idx)+\".csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mqTnscdxO6NS"
   },
   "source": [
    "## Perform sensitivity analysis of the PINN for the LGM\n",
    "\n",
    "Perform the training process for the LGM, for each of the model weights.\n",
    "\n",
    "Notice that training the neural network is resource expensive and may take minutes to hours depending on the computer facilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "EX7VK4zpO6NS",
    "outputId": "b135b048-f395-42cd-90d1-2e82de7eb4e7"
   },
   "outputs": [],
   "source": [
    "models_list = []\n",
    "list_name = []\n",
    "\n",
    "for idx, pde_weight in enumerate(weights):\n",
    "\n",
    "    north_mean = dde.Variable(-1.0)\n",
    "    south_mean = dde.Variable(-2.0)\n",
    "    D = dde.Variable(1.0)\n",
    "\n",
    "    bc_1 = dde.DirichletBC(geometry_rectangle, lambda x: north_mean, space_boundary_north)\n",
    "    bc_2 = dde.DirichletBC(geometry_rectangle, lambda x: south_mean, space_boundary_south)\n",
    "\n",
    "    periodic_condition = dde.icbc.PeriodicBC(geom = geometry_rectangle, component_x = 0, on_boundary = periodic_boundary, derivative_order= 0)\n",
    "    periodic_condition_derivative = dde.icbc.PeriodicBC(geom = geometry_rectangle, component_x = 0, on_boundary = periodic_boundary, derivative_order= 1)\n",
    "\n",
    "    data_observ_points_LGM, observe_u_LGM = training_points(df_empirical_LGM)\n",
    "\n",
    "    name = 'model_empirical_LGM_' + str(idx)\n",
    "    model_empirical_LGM, params, best_step = train_process(data_observ_points_LGM, observe_u_LGM, D, bc_1, bc_2, name, pde_weight)\n",
    "\n",
    "    models_list.append(model_empirical_LGM)\n",
    "    list_name.append(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TaIU2bJhO6NS"
   },
   "source": [
    "Apply the PINN to a global grid to reconstruct the dust deposition rates and save the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 246
    },
    "id": "hn95q4NGO6NS",
    "outputId": "ff6e107f-0254-4eb0-98f8-d4d934517f19"
   },
   "outputs": [],
   "source": [
    "for idx, model in enumerate(models_list):\n",
    "\n",
    "    mean = df_empirical_LGM['log_dep'].mean()\n",
    "    std = df_empirical_LGM['log_dep'].std()\n",
    "\n",
    "    # PINN reconstruction on the global grid\n",
    "    calculate_save_df(model, df_global_grid, mean, std, RESULTS_PATH, \"df_pinn_empirical_LGM_sensitivity_\"+str(idx)+\".csv\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
