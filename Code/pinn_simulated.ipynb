{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yl93P7bU_XDm"
   },
   "source": [
    "# PINN on simulated dust flux datasets\n",
    "\n",
    "This notebook trains the PINN on the simulated ERA5 dataset for gridded dust flux depositions. The PINN is then applied to reconstruct a global map of dust flux depositions.\n",
    "\n",
    "The preprocessing codes should have been performed before.\n",
    "\n",
    "The training of the PINN may take several hours, depending on the computer facilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MVAv8qoj6Eda"
   },
   "source": [
    "## Preliminaries\n",
    "\n",
    "Import the necessary libraries and specify the data folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vml3l0PulZNT",
    "outputId": "aae3bde4-8ae6-4b2f-ceeb-1ca240f606f0"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import deepxde as dde\n",
    "import geopandas as gpd\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from scipy.stats import norm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from deepxde.backend import tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YIFFlBjM7fTo"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wbU4LQjM0qa_"
   },
   "outputs": [],
   "source": [
    "DATA_PATH = \"../Data/\"\n",
    "INPUT_MODEL_PATH = DATA_PATH + \"processed_data/\"\n",
    "MODEL_SAVE_PATH = DATA_PATH + \"trained_models/\"\n",
    "RESULTS_PATH = DATA_PATH + \"model_results/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TbSNmZ-fuTAw"
   },
   "outputs": [],
   "source": [
    "world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IRmdNcPbqKqk"
   },
   "source": [
    "Load functions for training the PINN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8wSSRjXsqLAs"
   },
   "outputs": [],
   "source": [
    "with open(\"functions_training_model.py\", 'r') as file:\n",
    "    content = file.read()\n",
    "\n",
    "# Execute the content of the .py file\n",
    "exec(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qWyM1wVtYteV"
   },
   "source": [
    "## Load the datasets\n",
    "\n",
    "Load the datasets with dust flux depositions in the Holocene and LGM periods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fh3IhATdTqbI"
   },
   "outputs": [],
   "source": [
    "df_simulated_Holocene = pd.read_csv(INPUT_MODEL_PATH + \"df_simulation_Holocene.csv\")\n",
    "df_simulated_LGM = pd.read_csv(INPUT_MODEL_PATH + \"df_simulation_LGM.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yiv44aqi6Edd"
   },
   "source": [
    "Load the global grid with a 3 degrees resolution on which the dust flux deposition will be reconstructed with the PINN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cjx8t3jV6Edd"
   },
   "outputs": [],
   "source": [
    "df_global_grid = pd.read_csv(INPUT_MODEL_PATH + \"df_global_grid.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JNZru0umsmPd"
   },
   "source": [
    "## Specify the model settings\n",
    "\n",
    "The following steps are needed to set all the variables for the problem.\n",
    "1. Incorporate the wind directions.\n",
    "2. Define the training points.\n",
    "3. Define the equation with the domain and the boundary conditions.\n",
    "4. Define the neural network architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H4vx0yFDZcpb"
   },
   "source": [
    "### 1. Load the processed wind data\n",
    "\n",
    "The advection term includes the latitude-dependent average wind speed. The latitude and wind speed have to be normalized to be included in the PINN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cVq1MnC8A9xq"
   },
   "outputs": [],
   "source": [
    "df_wind = pd.read_csv(INPUT_MODEL_PATH + \"df_wind.csv\", usecols=['wind', 'latitude'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "prhVlY_PLwPM"
   },
   "outputs": [],
   "source": [
    "latitude_wind, mean_wind = df_wind['latitude'].values/90, df_wind['wind'].values/df_wind['wind'].max()\n",
    "\n",
    "def wind_latitude(latitude):\n",
    "    interpolated = wind_tf_interp(latitude, tf.convert_to_tensor(latitude_wind), tf.convert_to_tensor(mean_wind))\n",
    "    return interpolated\n",
    "\n",
    "tf_wind_latitude = tf.function(wind_latitude)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qpQHxmZtujGy"
   },
   "source": [
    "### 2. Define the training points\n",
    "\n",
    "The training points are the locations for wich dust deposition data is available. The latitude and longitude are normalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nc-bVyuNHiyt"
   },
   "outputs": [],
   "source": [
    "def training_points(df):\n",
    "\n",
    "    data_observ_points = dde.data.DataSet(\n",
    "        X_train = df[['lon', 'lat']].values/90,\n",
    "        y_train = df['log_dep_norm'].values.reshape(-1,1),\n",
    "        X_test = df[['lon', 'lat']].values/90,\n",
    "        y_test = df['log_dep_norm'].values.reshape(-1,1),\n",
    "        standardize=False)\n",
    "\n",
    "    observe_u  = dde.icbc.PointSetBC(data_observ_points.train_x,\n",
    "        df['log_dep_norm'].values.reshape(-1,1), component=0)\n",
    "\n",
    "    return data_observ_points, observe_u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zC-VUycebmQ3"
   },
   "source": [
    "### 3. Define the equation with the domain and the boundary conditions\n",
    "\n",
    "We model the Earth's surface as a sphere and write the advection-diffusion equation in spherical coordinates. Let denote the longitude by $\\lambda \\in (-\\pi, \\pi)$ and the latitude by $\\theta \\in (-\\frac{\\pi}{2}, -\\frac{\\pi}{2})$ in radians. The advection-diffusion equation is defined as\n",
    "\\begin{align}\n",
    "\t\\frac{v_1}{\\cos(\\theta)} \\frac{\\partial u}{\\partial \\lambda} - D \\left( \\frac{1}{\\cos^2(\\theta)} \\frac{\\partial^2 u}{\\partial \\lambda^2} + \\frac{\\partial^2 u}{\\partial \\theta ^2} - \\tan(\\theta) \\frac{\\partial u}{\\partial \\theta} \\right) &= 0,\n",
    "\\end{align}\n",
    "with the periodic boundary conditions\n",
    "\\begin{align}\n",
    "\\begin{cases}\n",
    "\tu(-\\pi, \\theta) = u(\\pi, \\theta) ,\\\\\n",
    "\t\\frac{\\partial u (-\\pi, \\theta)}{\\partial \\lambda} = \\frac{\\partial u  (\\pi, \\theta)}{\\partial \\lambda}.\n",
    "\\end{cases}\n",
    "\\end{align}\n",
    "in longitudinal direction. The values at the poles are prescribed as\n",
    "\\begin{cases}\n",
    "\tu(\\lambda, -\\frac{\\pi}{2}) = u_\\text{south}, \\\\\n",
    "\tu(\\lambda, \\frac{\\pi}{2}) = u_\\text{north}.\n",
    "\\end{cases}\n",
    "Notice that due to the singularity at the poles, the training domain will be taken from -81 to 81 degrees latitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5yZQGGQV9xdF"
   },
   "outputs": [],
   "source": [
    "x_min, x_max = -2.0, 2.0\n",
    "y_min, y_max = -0.89, 0.89\n",
    "\n",
    "left_corner = np.array([x_min, y_min]) # xmin, ymin – Coordinate of bottom left corner.\n",
    "right_corner = np.array([x_max, y_max]) # xmax, ymax – Coordinate of top right corner.\n",
    "geometry_rectangle = dde.geometry.geometry_2d.Rectangle(left_corner, right_corner)\n",
    "\n",
    "# Reduce the training domain to avoid pole singularities.\n",
    "df_simulated_Holocene_2 = df_simulated_Holocene[(df_simulated_Holocene['lat'] >= -81) & (df_simulated_Holocene['lat'] <= 81)]\n",
    "df_simulated_LGM_2 = df_simulated_LGM[(df_simulated_LGM['lat'] >= -81) & (df_simulated_LGM['lat'] <= 81)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rnk3u4_FLwLt"
   },
   "outputs": [],
   "source": [
    "D = dde.Variable(1.0)\n",
    "\n",
    "def pde(x, u):\n",
    "    du_x = dde.grad.jacobian(u, x, j=0) # du/dlambda\n",
    "    du_y = dde.grad.jacobian(u, x, j=1) # du/dtheta\n",
    "\n",
    "    K = wind_latitude(x[:, 1:2])\n",
    "    K = tf.cast(K, tf.float32)\n",
    "\n",
    "    du_xx = dde.grad.hessian(u, x, i=0, j=0) # d^2u/dlambda^2\n",
    "    du_yy = dde.grad.hessian(u, x, i=1, j=1) # d^2u/dtheta^2\n",
    "\n",
    "    return (\n",
    "        ( -K * du_x * (1/tf.cos(x[:, 1:2]* np.pi /2)) +\n",
    "        D *( (1/(tf.cos(x[:, 1:2]* np.pi /2)**2) * du_xx + du_yy - tf.tan(x[:, 1:2]* np.pi /2) * du_y)) )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LYOKDP3C4L8B"
   },
   "outputs": [],
   "source": [
    "def space_boundary_north(x, on_boundary):\n",
    "    return on_boundary and np.isclose(y_max, x[1])\n",
    "\n",
    "def space_boundary_south(x, on_boundary):\n",
    "    return on_boundary and np.isclose(y_min, x[1])\n",
    "\n",
    "def periodic_boundary(x, domain):\n",
    "    return domain and (np.isclose(x[0], x_min) or np.isclose(x[0], x_max))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B21USxArvLQw"
   },
   "source": [
    "### 4. Define the neural network architecture\n",
    "\n",
    "The following model parameters need to be set.\n",
    "\n",
    "* `data_observ_points` and `observe_u`: The training locations and the dust measurements.\n",
    "\n",
    "* `MODEL_NAME_TO_SAVE`: The name under which the trained model will be saved.\n",
    "\n",
    "* `geometry_rectangle`: The geometry object defining the domain of the PDE problem.\n",
    "\n",
    "* `pde`: The PDE equation function defining the PDE problem to be solved.\n",
    "\n",
    "* `periodic_condition`, `periodic_condition_derivative`, `bc_1`, and `bc_2`: These are functions representing different boundary conditions.\n",
    "\n",
    "* `num_domain` and `num_boundary`: The number of domain and boundary points to use in the training data.\n",
    "\n",
    "* `neurons`, `layer`, `layer_size`, `activation`, and `initializer`: These parameters define the architecture of the neural network model (FNN) used to approximate the solution. They determine the number of hidden neurons, layers, activation function, and weight initialization method.\n",
    "\n",
    "* `epochs`: The number of training epochs to run.\n",
    "\n",
    "* `lr`: Learning rate for the Adam optimizer.\n",
    "\n",
    "* `D`: This variable represents the diffusion coefficient. It's a hyperparameter that affects the rate of diffusion in the PDE problem. This value is calculated by the network as an inverse problem\n",
    "\n",
    "* `external_trainable_variables`: A list of external variables (like `D`, `north_mean`, `south_mean`) that the model should optimize during training.\n",
    "\n",
    "* `loss_weights`: Weights applied to different loss terms in the optimization objective.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F0mr1s08qx_j"
   },
   "outputs": [],
   "source": [
    "def train_process(data_observ_points, observe_u, D, bc_1, bc_2, model_name):\n",
    "    data = dde.data.PDE(\n",
    "        geometry_rectangle,\n",
    "        pde,\n",
    "        [observe_u, periodic_condition, periodic_condition_derivative, bc_1, bc_2],\n",
    "        num_domain=2592,\n",
    "        num_boundary=216,\n",
    "        anchors=data_observ_points.train_x,\n",
    "        train_distribution='uniform'\n",
    "    )\n",
    "\n",
    "    neurons = 32\n",
    "    layer = 5\n",
    "    layer_size = [2] + [neurons] * layer + [1]\n",
    "    activation = \"selu\"\n",
    "    initializer = \"Glorot normal\"\n",
    "    net = dde.maps.FNN(layer_size, activation, initializer)\n",
    "    model = dde.Model(data, net)\n",
    "    dde.optimizers.set_LBFGS_options(maxcor=50, ftol=1e-20, maxiter=1e5)\n",
    "    model.compile(\"adam\", lr=0.00001, external_trainable_variables=[D, north_mean, south_mean],\n",
    "                  loss_weights=[1, 10, 0.5, 0.5, 1, 1])\n",
    "\n",
    "    # Train and save the model\n",
    "    checkpointer = dde.callbacks.ModelCheckpoint(\n",
    "        f\"{MODEL_SAVE_PATH}{model_name}/{model_name}.ckpt\",\n",
    "        verbose=0, period=10000,\n",
    "    )\n",
    "\n",
    "    variable = dde.callbacks.VariableValue([D, north_mean, south_mean], period=10000,\n",
    "                                           filename=MODEL_SAVE_PATH+model_name+\"/variables.dat\")\n",
    "\n",
    "    losshistory, train_state = model.train(epochs=150000, callbacks=[variable, checkpointer])\n",
    "    dde.saveplot(losshistory, train_state, issave=False, isplot=True)\n",
    "    params = variable.get_value()\n",
    "\n",
    "    return model, params, train_state.best_step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nrQfeN85etQ0"
   },
   "source": [
    "## Perform the training process of the PINN\n",
    "\n",
    "Perform the training process for the Holocene and LGM datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Ru62z5uXkShW",
    "outputId": "5410f885-a404-4474-9474-46640a66643e",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "north_mean = dde.Variable(-1.0)\n",
    "south_mean = dde.Variable(-2.0)\n",
    "D = dde.Variable(1.0)\n",
    "\n",
    "bc_1 = dde.DirichletBC(geometry_rectangle, lambda x: north_mean, space_boundary_north)\n",
    "bc_2 = dde.DirichletBC(geometry_rectangle, lambda x: south_mean, space_boundary_south)\n",
    "\n",
    "periodic_condition = dde.icbc.PeriodicBC(geom = geometry_rectangle, component_x = 0, on_boundary = periodic_boundary, derivative_order= 0)\n",
    "periodic_condition_derivative = dde.icbc.PeriodicBC(geom = geometry_rectangle, component_x = 0, on_boundary = periodic_boundary, derivative_order= 1)\n",
    "\n",
    "data_observ_points_holocene, observe_u_holocene = training_points(df_simulated_Holocene_2)\n",
    "model_simulated_Holocene, params, best_step = train_process(data_observ_points_holocene, observe_u_holocene, D, bc_1, bc_2, 'model_simulated_Holocene')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "qlm9yS_svG8n",
    "outputId": "251543fe-fbc8-4cde-c442-f6470ce82a87",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "north_mean = dde.Variable(-1.0)\n",
    "south_mean = dde.Variable(-2.0)\n",
    "D = dde.Variable(1.0)\n",
    "\n",
    "bc_1 = dde.DirichletBC(geometry_rectangle, lambda x: north_mean, space_boundary_north)\n",
    "bc_2 = dde.DirichletBC(geometry_rectangle, lambda x: south_mean, space_boundary_south)\n",
    "\n",
    "periodic_condition = dde.icbc.PeriodicBC(geom = geometry_rectangle, component_x = 0, on_boundary = periodic_boundary, derivative_order= 0)\n",
    "periodic_condition_derivative = dde.icbc.PeriodicBC(geom = geometry_rectangle, component_x = 0, on_boundary = periodic_boundary, derivative_order= 1)\n",
    "\n",
    "data_observ_points_LGM, observe_u_LGM = training_points(df_simulated_LGM_2)\n",
    "model_simulated_LGM, params, best_step = train_process(data_observ_points_LGM, observe_u_LGM, D, bc_1, bc_2, 'model_simulated_LGM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A5BJl565zSPR"
   },
   "source": [
    "## Reconstruct the global dust deposition with the PINN\n",
    "\n",
    "Apply the PINN to a global grid to reconstruct the dust deposition rates and save the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7qXyLMZp2jrW"
   },
   "outputs": [],
   "source": [
    "def calculate_save_df(model, df_to_predict, mean, std, path, filename):\n",
    "    \"Perform the predictions and save the results.\"\n",
    "    U_pred = model.predict(df_to_predict[['lon', 'lat']].values/90)\n",
    "    U_pred_denorm = (U_pred + mean)*std\n",
    "    # Add the value of the prediction of the PINN in the specific location\n",
    "    df_to_predict['PINN_log_dep'] = U_pred_denorm\n",
    "\n",
    "    # Save the dataset with the prediction\n",
    "    with open(path+filename, 'w') as f:\n",
    "        df_to_predict.to_csv(f, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nqek9ter6Edg"
   },
   "outputs": [],
   "source": [
    "mean = df_simulated_Holocene['log_dep'].mean()\n",
    "std = df_simulated_Holocene['log_dep'].std()\n",
    "\n",
    "calculate_save_df(model_simulated_Holocene, df_simulated_Holocene, mean, std, RESULTS_PATH, \"df_pinn_simulated_Holocene.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eVYQt6LN6Edg"
   },
   "outputs": [],
   "source": [
    "mean = df_simulated_LGM['log_dep'].mean()\n",
    "std = df_simulated_LGM['log_dep'].std()\n",
    "\n",
    "calculate_save_df(model_simulated_LGM, df_simulated_LGM, mean, std, RESULTS_PATH, \"df_pinn_simulated_LGM.csv\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
